{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import time  # to time our operations\n",
    "from csv import writer\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "i = 0\n",
    "stem = 'https://www.boston.gov'\n",
    "t = time()\n",
    "topics = []\n",
    "dates = []\n",
    "text = []\n",
    "s = requests.Session()\n",
    "\n",
    "while i < 13:\n",
    "\n",
    "    response = s.get('https://www.boston.gov/news?page={}'.format(i))\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    posts= soup.find_all(class_='mobile-1-col desktop-60-left news-left')\n",
    "\n",
    "    links = []\n",
    "\n",
    "\n",
    "    for post in posts:\n",
    "        links.append(post.find('a')['href'])\n",
    "\n",
    "    print(links)\n",
    "\n",
    "    for link in links:\n",
    "        link = stem + link\n",
    "        sub_response = requests.get(link)\n",
    "        soup1 = BeautifulSoup(sub_response.text, 'html.parser')\n",
    "        article = soup1.find(class_='field-item even')\n",
    "        for paragraph in article.find_all('p'):\n",
    "            text.append(paragraph.text.replace(';', ''))\n",
    "        text.append('*******')\n",
    "        print('Page {}: the news {} has been processed'.format(i+1, link))\n",
    "\n",
    "    i+=1\n",
    "\n",
    "text2 = []\n",
    "a =''\n",
    "j=0\n",
    "while j <len(text):\n",
    "    if text[j] != '*******':\n",
    "        a = a + text[j]\n",
    "    else:\n",
    "        text2.append(a)\n",
    "        a =''\n",
    "    j+=1\n",
    "print(text2)\n",
    "print(len(text2))\n",
    "\n",
    "orig_stdout = sys.stdout\n",
    "f = open('Boston_0.txt', 'w')\n",
    "sys.stdout = f\n",
    "\n",
    "for j in range(len(text2)):\n",
    "    if len(text2[j])>40:\n",
    "        a = text2[j].encode('ascii', 'ignore')\n",
    "        print(a.decode('ascii'))\n",
    "\n",
    "sys.stdout = orig_stdout\n",
    "f.close()\n",
    "\n",
    "print('Time to process data: {} mins'.format(round((time() - t) / 60, 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helsinki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import time  # to time our operations\n",
    "import sys\n",
    "import urllib3\n",
    "\n",
    "i = 1\n",
    "stem = 'https://www.hel.fi'\n",
    "t = time()\n",
    "text = []\n",
    "s = requests.Session()\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "\n",
    "while i < 6: #number of pages\n",
    "\n",
    "    response = s.get('https://www.hel.fi/wps/portal/HelsinkiV2/!ut/p/z1/jZBNC4JAFEV_jUt7b_wYx6UZVAsjSDFnM4w6ilCToBb461P3Wm934ZwL9wGHO3At300t--al5WPKGaeCEkYJYVbEksTBYH-6WH58sBEtSBcAVy5A4P_4GwDfrs8m31v1jwg34MDTMBLXM2RkDroTm4pYaFmrHSsJ-sxH0yupazpVLk3fdQuTWAWqCmlu5wwyOpeG8fQqA4ehbzrVG6i0gVp9umkg_7WhfSb3MVb5WLXBF9wyrAE!/p0/IZ7_61861182M8UU40ABHN29TD30G0=CZ6_61861182M8UU40ABHN29TD3002=MEns_Z7_61861182M8UU40ABHN29TD30G0_WCM_Page.8d109890-7d65-4fba-955c-12c0ef06b3b8!{}=CTX!QCPuutisetQCPenQCPnews=WCM_PI!1==/'.format(i))\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    posts = soup.find_all(class_='news-topic-list-item')\n",
    "\n",
    "    links = []\n",
    "\n",
    "    exception = 'https://asiointi.hel.fi/wps/portal/sahkoinenasiointi/tiedote?wcm_p=/sahkoinenasiointiv2/tiedotteet/omaolo-palvelut&presentationtemplate=tiedote-en'\n",
    "    \n",
    "    for post in posts:\n",
    "        if post.find('a')['href'] != exception:\n",
    "            links.append(post.find('a')['href'])\n",
    "        \n",
    "    for link in links:\n",
    "        link = stem + link\n",
    "        sub_response = requests.get(link)\n",
    "        soup1 = BeautifulSoup(sub_response.text, 'html.parser')\n",
    "        article = soup1.find(class_='news-main-item-content')\n",
    "        for paragraph in article.find_all('p'):\n",
    "            text.append(paragraph.text.replace(';', ''))\n",
    "\n",
    "        text.append('*******')\n",
    "        print('Page {}: the news {} has been processed'.format(i, link))\n",
    "\n",
    "    i+=1\n",
    "\n",
    "\n",
    "text2 = []\n",
    "a =''\n",
    "j=0\n",
    "while j <len(text):\n",
    "    if text[j] != '*******':\n",
    "        a = a + text[j]\n",
    "    else:\n",
    "        text2.append(a)\n",
    "        a =''\n",
    "    j+=1\n",
    "print(text2)\n",
    "print(len(text2))\n",
    "\n",
    "c=[]\n",
    "for j in range(len(text2)):\n",
    "    c.append(len(text2[j]))\n",
    "\n",
    "print(c)\n",
    "\n",
    "orig_stdout = sys.stdout\n",
    "f = open('Helsinki_0.txt', 'w')\n",
    "sys.stdout = f\n",
    "\n",
    "for j in range(len(text2)):\n",
    "    if len(text2[j]) > 40:\n",
    "        a = text2[j].encode('ascii', 'ignore')\n",
    "        print(a.decode('ascii'))\n",
    "\n",
    "sys.stdout = orig_stdout\n",
    "f.close()\n",
    "\n",
    "print('Time to process data: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seoul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import time  # to time our operations\n",
    "import sys\n",
    "\n",
    "i = 1\n",
    "t = time()\n",
    "text = []\n",
    "s = requests.Session()\n",
    "\n",
    "\n",
    "while i < 4: #number of pages\n",
    "\n",
    "    response = s.get('http://english.seoul.go.kr/category/get-to-know-us/seoul-news/city-news/page/{}/'.format(i))\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    posts = soup.find_all(\"div\", {\"class\":\"template_cnt\"})\n",
    "\n",
    "    links = []\n",
    "\n",
    "    for post in posts:\n",
    "        links.append(post.find('a')['href'])\n",
    "\n",
    "    for link in links:\n",
    "        sub_response = requests.get(link)\n",
    "        soup1 = BeautifulSoup(sub_response.text, 'html.parser')\n",
    "        article = soup1.find(class_='view_contents')\n",
    "        if article:\n",
    "            for paragraph in article.find_all('p'):\n",
    "                text.append(paragraph.text.replace(';', ''))\n",
    "        text.append('*******')\n",
    "        print('Page {}: the news {} has been processed'.format(i, link))\n",
    "\n",
    "    i+=1\n",
    "\n",
    "\n",
    "text2 = []\n",
    "a =''\n",
    "j=0\n",
    "while j <len(text):\n",
    "    if text[j] != '*******':\n",
    "        a = a + text[j]\n",
    "    else:\n",
    "        text2.append(a)\n",
    "        a =''\n",
    "    j+=1\n",
    "print(text2)\n",
    "print(len(text2))\n",
    "\n",
    "orig_stdout = sys.stdout\n",
    "f = open('Seoul_0.txt', 'w')\n",
    "sys.stdout = f\n",
    "\n",
    "for j in range(len(text2)):\n",
    "    if len(text2[j]) > 500:\n",
    "        a = text2[j].encode('ascii', 'ignore')\n",
    "        print(a.decode('ascii'))\n",
    "\n",
    "sys.stdout = orig_stdout\n",
    "f.close()\n",
    "\n",
    "print('Time to process data: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taipei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import time  # to time our operations\n",
    "#from lxml import html\n",
    "#import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "i = 1\n",
    "stem = 'https://english.rdec.gov.taipei/'\n",
    "t = time()\n",
    "text = []\n",
    "s = requests.Session()\n",
    "\n",
    "\n",
    "while i <7: #number of pages\n",
    "\n",
    "    response = s.get('https://english.rdec.gov.taipei/News.aspx?n=50819E2E63622F17&sms=DFFA119D1FD5602C&page={}&PageSize=20'.format(i))\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    posts = soup.find_all(\"td\", {\"class\": \"CCMS_jGridView_td_Class_1\"})\n",
    "\n",
    "    links = []\n",
    "\n",
    "    for post in posts:\n",
    "        links.append(post.find('a')['href'])\n",
    "\n",
    "    print(links)\n",
    "\n",
    "    for link in links:\n",
    "        link = stem + link\n",
    "        sub_response = requests.get(link)\n",
    "        soup1 = BeautifulSoup(sub_response.text, 'html.parser')\n",
    "        article = soup1.find(class_='essay')\n",
    "        for paragraph in article.find_all('span'):\n",
    "            text.append(paragraph.text.replace(';', ''))\n",
    "\n",
    "        print('Page {}: the news {} has been processed'.format(i, link))\n",
    "\n",
    "    i+=1\n",
    "\n",
    "orig_stdout = sys.stdout\n",
    "f = open('Taipei_0.txt', 'w')\n",
    "sys.stdout = f\n",
    "\n",
    "for j in range(len(text)):\n",
    "    a = text[j].encode('ascii', 'ignore')\n",
    "    print(a.decode('ascii'))\n",
    "\n",
    "sys.stdout = orig_stdout\n",
    "f.close()\n",
    "\n",
    "print('Time to process data: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
