{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import os\n",
    "from time import time  # to time our operations\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_city(City):\n",
    "    month = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december' ]\n",
    "    day = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "    if City == 'Boston':\n",
    "        return month + day + ['boston', 'roxbury','fiandaca','opencounter', 'accela','forth', 'ortega', 'burro','theroxburyinnovation', 'julie', 'elliott', '21stcentury', 'nstar','dorsey','austin', 'blackmon', 'swett', 'ofboston', 'paired','ass','first', 'martin', 'addition', 'every', 'walsh', 'according', 'please visit', 'noted',\n",
    "            'please','april', 'february', 'visit', 'across', 'december', 'https', 'following',\n",
    "             'noted', 'along', 'three', 'total', 'pointed', 'citys', 'announced', 'massachusetts', 'million']\n",
    "    if City =='Taipei':\n",
    "        return month + day +['taipei','since', 'dadaocheng', 'serf', 'addition', 'february','related', 'october', 'january', 'around', 'taking', 'march', 'first', 'according', 'please visit',\n",
    "                'noted', 'please','april', 'among''february', 'visit', 'across', 'december', 'https', 'ko',\n",
    "                'following', 'universiade','september','november','august','noted', 'along', 'three', 'total', 'pointed', 'citys', 'chinese']\n",
    "    if City == 'Seoul':\n",
    "        return month + day +['seoul','yeomchang', 'dulle','three', 'gaehwasinnonhyeon', 'major', 'first', 'million', 'various','selected','around','number', 'based','total',\n",
    "                'order','special', 'since']\n",
    "    if City == 'Helsinki':\n",
    "        return month + day + ['helsinki','based', '6aika', 'wrtsil','january', 'first', 'sitra']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus(City):\n",
    "    folder = 'C:/Users/user/PycharmProjects/DataCamp/{}_Data'.format(City)\n",
    "\n",
    "    txt = ''\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords.extend(stopwords_city(City))\n",
    "    corpus1 = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for file in os.listdir(folder):\n",
    "        path = os.path.join(folder, file)\n",
    "        book = open(path, 'r')\n",
    "        text = book.read()\n",
    "        #data = text.split('.')\n",
    "        data = text.splitlines()\n",
    "\n",
    "        for sentence in data:\n",
    "            shortword = re.compile(r'\\W*\\b\\w{1,4}\\b')\n",
    "            sentence = shortword.sub('', sentence).lower()\n",
    "            word_list = tokenizer.tokenize(sentence.lower())\n",
    "            word_list1 = [word for word in word_list if word.isalpha()]\n",
    "            word_list2 = [word for word in word_list1 if word not in stopwords]\n",
    "            corpus1.append(word_list2)\n",
    "\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(corpus1, min_count=5, threshold=100)  # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[corpus1], threshold=100)\n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    def make_bigrams(texts):\n",
    "        return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "    def make_trigrams(texts):\n",
    "        return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "    corpus2 = make_bigrams(corpus1)\n",
    "\n",
    "    #print(corpus2[:10])\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for w in range(len(corpus2)):\n",
    "        x = 0\n",
    "        while x <(len(corpus2[w])):\n",
    "            corpus2[w][x] = lemmatizer.lemmatize(corpus2[w][x])\n",
    "            if corpus2[w][x] in stopwords:\n",
    "                del corpus2[w][x]\n",
    "            x += 1\n",
    "\n",
    "    return corpus2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.081*\"health\" + 0.065*\"service\" + 0.037*\"shelter\" + 0.030*\"homeless\" + '\n",
      "  '0.029*\"housing\" + 0.028*\"recovery\" + 0.021*\"individual\" + '\n",
      "  '0.020*\"prevention\" + 0.018*\"homelessness\" + 0.017*\"emergency\"'),\n",
      " (1,\n",
      "  '0.112*\"housing\" + 0.056*\"development\" + 0.039*\"affordable\" + 0.034*\"unit\" + '\n",
      "  '0.020*\"income\" + 0.017*\"neighborhood\" + 0.013*\"building\" + 0.013*\"senior\" + '\n",
      "  '0.012*\"property\" + 0.012*\"cost\"'),\n",
      " (2,\n",
      "  '0.031*\"resident\" + 0.023*\"department\" + 0.013*\"parking\" + '\n",
      "  '0.013*\"transportation\" + 0.012*\"neighborhood\" + 0.011*\"safety\" + '\n",
      "  '0.010*\"process\" + 0.010*\"information\" + 0.008*\"mayor\" + 0.008*\"permit\"'),\n",
      " (3,\n",
      "  '0.062*\"energy\" + 0.061*\"city\" + 0.043*\"climate\" + 0.039*\"election\" + '\n",
      "  '0.038*\"action\" + 0.027*\"building\" + 0.025*\"climate_change\" + 0.025*\"voter\" '\n",
      "  '+ 0.019*\"summit\" + 0.017*\"environment\"'),\n",
      " (4,\n",
      "  '0.052*\"finding\" + 0.030*\"concert\" + 0.022*\"james\" + 0.020*\"latin\" + '\n",
      "  '0.017*\"patriot\" + 0.017*\"patient\" + 0.017*\"incident\" + 0.015*\"hospital\" + '\n",
      "  '0.015*\"signature\" + 0.015*\"senator\"'),\n",
      " (5,\n",
      "  '0.025*\"river\" + 0.012*\"everett\" + 0.009*\"stephanie\" + 0.000*\"metro\" + '\n",
      "  '0.000*\"gaming\" + 0.000*\"shield\" + 0.000*\"mccabe\" + 0.000*\"bike\" + '\n",
      "  '0.000*\"parcel\" + 0.000*\"corruption\"'),\n",
      " (6,\n",
      "  '0.040*\"people\" + 0.017*\"together\" + 0.014*\"world\" + 0.012*\"digital\" + '\n",
      "  '0.012*\"future\" + 0.012*\"spirit\" + 0.011*\"right\" + 0.011*\"great\" + '\n",
      "  '0.011*\"idea\" + 0.011*\"would\"'),\n",
      " (7,\n",
      "  '0.054*\"project\" + 0.044*\"space\" + 0.030*\"design\" + 0.027*\"building\" + '\n",
      "  '0.027*\"public\" + 0.024*\"street\" + 0.019*\"historic\" + 0.017*\"square\" + '\n",
      "  '0.014*\"community\" + 0.013*\"neighborhood\"'),\n",
      " (8,\n",
      "  '0.018*\"council\" + 0.017*\"award\" + 0.017*\"cultural\" + 0.017*\"mayor\" + '\n",
      "  '0.016*\"artist\" + 0.013*\"community\" + 0.012*\"culture\" + 0.010*\"office\" + '\n",
      "  '0.009*\"public\" + 0.009*\"exhibition\"'),\n",
      " (9,\n",
      "  '0.037*\"landmarks_commission\" + 0.030*\"industrial\" + 0.029*\"application\" + '\n",
      "  '0.022*\"marriage\" + 0.020*\"fourth\" + 0.014*\"information\" + 0.014*\"street\" + '\n",
      "  '0.014*\"urban\" + 0.014*\"public\" + 0.013*\"cultural\"'),\n",
      " (10,\n",
      "  '0.085*\"budget\" + 0.051*\"investment\" + 0.038*\"imagine\" + 0.032*\"capital\" + '\n",
      "  '0.032*\"fiscal\" + 0.027*\"upgrade\" + 0.022*\"billion\" + 0.020*\"finance\" + '\n",
      "  '0.019*\"elderly_disabled\" + 0.014*\"venture\"'),\n",
      " (11,\n",
      "  '0.353*\"street\" + 0.029*\"traffic\" + 0.023*\"avenue\" + 0.019*\"parking\" + '\n",
      "  '0.014*\"pedestrian\" + 0.014*\"facebook\" + 0.013*\"right\" + 0.013*\"south\" + '\n",
      "  '0.012*\"patrick\" + 0.012*\"washington\"'),\n",
      " (12,\n",
      "  '0.106*\"police\" + 0.085*\"question\" + 0.039*\"teaching\" + 0.039*\"crime\" + '\n",
      "  '0.033*\"illegal\" + 0.028*\"regarding\" + 0.023*\"enforcement\" + '\n",
      "  '0.022*\"immigration\" + 0.019*\"craft\" + 0.017*\"class\"'),\n",
      " (13,\n",
      "  '0.006*\"temple\" + 0.000*\"gospel\" + 0.000*\"gospelfest\" + 0.000*\"praise\" + '\n",
      "  '0.000*\"ministry\" + 0.000*\"hairston\" + 0.000*\"stellar\" + 0.000*\"youthful\" + '\n",
      "  '0.000*\"album\" + 0.000*\"choir\"'),\n",
      " (14,\n",
      "  '0.051*\"mayor\" + 0.024*\"public\" + 0.015*\"service\" + 0.015*\"office\" + '\n",
      "  '0.011*\"member\" + 0.010*\"chief\" + 0.010*\"state\" + 0.010*\"today\" + '\n",
      "  '0.010*\"year\" + 0.009*\"national\"'),\n",
      " (15,\n",
      "  '0.106*\"center\" + 0.080*\"family\" + 0.067*\"event\" + 0.036*\"summer\" + '\n",
      "  '0.036*\"child\" + 0.027*\"childrens\" + 0.023*\"activity\" + 0.023*\"community\" + '\n",
      "  '0.020*\"youth\" + 0.018*\"information\"'),\n",
      " (16,\n",
      "  '0.041*\"community\" + 0.032*\"program\" + 0.025*\"mayor\" + 0.019*\"business\" + '\n",
      "  '0.018*\"youth\" + 0.018*\"opportunity\" + 0.013*\"neighborhood\" + '\n",
      "  '0.012*\"support\" + 0.011*\"initiative\" + 0.011*\"people\"'),\n",
      " (17,\n",
      "  '0.274*\"library\" + 0.192*\"public\" + 0.041*\"central\" + 0.024*\"service\" + '\n",
      "  '0.023*\"program\" + 0.022*\"collection\" + 0.021*\"copley_square\" + '\n",
      "  '0.018*\"branch\" + 0.018*\"established\" + 0.017*\"people\"'),\n",
      " (18,\n",
      "  '0.172*\"woman\" + 0.046*\"girl\" + 0.032*\"sport\" + 0.030*\"league\" + '\n",
      "  '0.027*\"discrimination\" + 0.027*\"legislation\" + 0.021*\"landlord\" + '\n",
      "  '0.018*\"civil_right\" + 0.017*\"player\" + 0.015*\"legislature\"'),\n",
      " (19,\n",
      "  '0.283*\"school\" + 0.133*\"student\" + 0.068*\"education\" + 0.036*\"committee\" + '\n",
      "  '0.033*\"superintendent\" + 0.025*\"teacher\" + 0.024*\"public\" + '\n",
      "  '0.021*\"learning\" + 0.017*\"child\" + 0.015*\"parent\"')]\n",
      "Perplexity:  -11.975653001504762\n",
      "Coherence Score:  0.4856800486186851\n",
      "Time to process Boston data: 1.35 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "City = 'Boston' ##Input\n",
    "data_lemmatized = corpus(City)\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "id2word.save('dictionary.gensim.{}'.format(City))\n",
    "texts = data_lemmatized\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                id2word=id2word,\n",
    "                                                num_topics=20,\n",
    "                                                random_state=100,\n",
    "                                                update_every=1,\n",
    "                                                chunksize=100,\n",
    "                                                passes=10,\n",
    "                                                alpha='auto',\n",
    "                                                per_word_topics=True)\n",
    "\n",
    "lda_model.save('model3.gensim.{}'.format(City))\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n",
    "\n",
    "print('Perplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)\n",
    "\n",
    "\n",
    "print('Time to process {} data: {} mins'.format(City, round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os \n",
    "%matplotlib inline\n",
    "import pyLDAvis.gensim\n",
    "import gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "d = gensim.corpora.Dictionary.load('dictionary.gensim.Boston')\n",
    "c = [id2word.doc2bow(text) for text in texts]\n",
    "lda = gensim.models.LdaModel.load('model3.gensim.Boston')\n",
    "\n",
    "pd.options.display.max_colwidth = 5000\n",
    "data = pyLDAvis.gensim.prepare(lda, c, d, mds='tsne')\n",
    "pyLDAvis.show(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(data, 'Boston.20.lda.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
