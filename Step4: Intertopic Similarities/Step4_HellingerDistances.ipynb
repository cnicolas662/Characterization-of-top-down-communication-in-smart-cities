{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/clementnicolas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/socks.py:58: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Callable\n",
      "/anaconda3/lib/python3.7/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import os\n",
    "import fnmatch\n",
    "import io\n",
    "from pprint import pprint\n",
    "from time import time  # to time our operations\n",
    "\n",
    "#LDA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os \n",
    "%matplotlib inline\n",
    "import pyLDAvis.gensim\n",
    "import gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess, tokenize\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "City = \"Taipei\"\n",
    "NumTopics=11\n",
    "\n",
    "lda = gensim.models.LdaModel.load('lda_final.{}'.format(City))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "topic1, topic2, topic3, topic4, topic5, topic6, topic7, topic8, topic9,topic10, topic11 = lda.show_topics(11,100)\n",
    "#, topic10, topic11, topic12, topic13\n",
    "Topics = [topic1, topic2, topic3, topic4, topic5, topic6, topic7, topic8, topic9,topic10, topic11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " '0.026*\"road\" + 0.021*\"station\" + 0.013*\"area\" + 0.012*\"park\" + 0.012*\"parking\" + 0.010*\"water\" + 0.009*\"traffic\" + 0.009*\"route\" + 0.009*\"take\" + 0.009*\"line\" + 0.008*\"transportation\" + 0.008*\"public\" + 0.007*\"operation\" + 0.007*\"stop\" + 0.007*\"fire\" + 0.007*\"period\" + 0.007*\"bridge\" + 0.006*\"light\" + 0.006*\"riverside\" + 0.006*\"hour\" + 0.006*\"train\" + 0.006*\"bicycle\" + 0.006*\"driver\" + 0.006*\"street\" + 0.006*\"measure\" + 0.006*\"time\" + 0.005*\"section\" + 0.005*\"office\" + 0.005*\"also\" + 0.005*\"agency\" + 0.005*\"lane\" + 0.005*\"vehicle\" + 0.005*\"cycling\" + 0.005*\"designated\" + 0.005*\"department\" + 0.005*\"exercise\" + 0.004*\"holiday\" + 0.004*\"police\" + 0.004*\"emergency\" + 0.004*\"year\" + 0.004*\"located\" + 0.004*\"minute\" + 0.004*\"intersection\" + 0.004*\"reminds\" + 0.004*\"minzu\" + 0.004*\"safety\" + 0.004*\"drill\" + 0.004*\"avoid\" + 0.004*\"alley_lane\" + 0.004*\"bus\" + 0.004*\"main\" + 0.004*\"wheel\" + 0.003*\"closed\" + 0.003*\"district\" + 0.003*\"staircase\" + 0.003*\"starting\" + 0.003*\"exit\" + 0.003*\"center\" + 0.003*\"include\" + 0.003*\"rescue\" + 0.003*\"east\" + 0.003*\"disaster_prevention\" + 0.003*\"location\" + 0.003*\"announced\" + 0.003*\"zhongshan\" + 0.003*\"neihu\" + 0.003*\"site\" + 0.003*\"zone\" + 0.003*\"direction\" + 0.003*\"shuttle\" + 0.003*\"next\" + 0.003*\"bike\" + 0.003*\"traveling\" + 0.003*\"facility\" + 0.003*\"schedule\" + 0.003*\"midnight\" + 0.003*\"hall\" + 0.003*\"nangang\" + 0.003*\"north\" + 0.003*\"ensure\" + 0.003*\"songshan\" + 0.003*\"venue\" + 0.002*\"vicinity\" + 0.002*\"service\" + 0.002*\"adjustment\" + 0.002*\"side\" + 0.002*\"place\" + 0.002*\"xinyi\" + 0.002*\"disaster\" + 0.002*\"effect\" + 0.002*\"evacuation\" + 0.002*\"right\" + 0.002*\"well\" + 0.002*\"call\" + 0.002*\"maintenance\" + 0.002*\"city\" + 0.002*\"work\" + 0.002*\"taxi\" + 0.002*\"transfer\" + 0.002*\"riverside_parks\"')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6111524905337291 0.7080060856446175 0.7157839585923855\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, sparse2full\n",
    "\n",
    "def make_topics_bow(topic):\n",
    "    # takes the string returned by model.show_topics()\n",
    "    # split on strings to get topics and the probabilities\n",
    "    topic = topic.split('+')\n",
    "    # list to store topic bows\n",
    "    topic_bow = []\n",
    "    for word in topic:\n",
    "        # split probability and word\n",
    "        prob, word = word.split('*')\n",
    "        # get rid of spaces\n",
    "        word = word.split('\"')[1]\n",
    "        word = word.replace(\" \",'')\n",
    "        #print(word)\n",
    "        # convert to word_type\n",
    "        word = lda.id2word.doc2bow([word])[0][0]\n",
    "        topic_bow.append((word, float(prob)))\n",
    "    return topic_bow\n",
    "\n",
    "\n",
    "topic_1 = make_topics_bow(topic1[1])\n",
    "topic_2 = make_topics_bow(topic2[1])\n",
    "topic_3 = make_topics_bow(topic3[1])\n",
    "topic_4 = make_topics_bow(topic4[1])\n",
    "\n",
    "print(hellinger(topic_1, topic_2), hellinger(topic_1, topic_3), hellinger(topic_1, topic_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.61 0.71 0.72 0.74 0.64 0.72 0.68 0.68 0.59 0.66 \n",
      "0.61 0.0 0.67 0.69 0.74 0.62 0.7 0.68 0.64 0.55 0.63 \n",
      "0.71 0.67 0.0 0.75 0.79 0.65 0.74 0.73 0.68 0.69 0.71 \n",
      "0.72 0.69 0.75 0.0 0.76 0.72 0.75 0.73 0.7 0.68 0.74 \n",
      "0.74 0.74 0.79 0.76 0.0 0.71 0.78 0.76 0.73 0.75 0.79 \n",
      "0.64 0.62 0.65 0.72 0.71 0.0 0.71 0.69 0.66 0.65 0.67 \n",
      "0.72 0.7 0.74 0.75 0.78 0.71 0.0 0.73 0.72 0.7 0.74 \n",
      "0.68 0.68 0.73 0.73 0.76 0.69 0.73 0.0 0.71 0.67 0.72 \n",
      "0.68 0.64 0.68 0.7 0.73 0.66 0.72 0.71 0.0 0.67 0.71 \n",
      "0.59 0.55 0.69 0.68 0.75 0.65 0.7 0.67 0.67 0.0 0.62 \n",
      "0.66 0.63 0.71 0.74 0.79 0.67 0.74 0.72 0.71 0.62 0.0 \n"
     ]
    }
   ],
   "source": [
    "for topicA in Topics:\n",
    "    for topicB in Topics:\n",
    "        topic_A = make_topics_bow(topicA[1])\n",
    "        topic_B = make_topics_bow(topicB[1])\n",
    "        print(round(hellinger(topic_A,topic_B),2), end = ' ')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5038763526486121"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hellinger(make_topics_bow(topic4[1]), make_topics_bow(topic11[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, '0.043*\"street\" + 0.008*\"event\" + 0.007*\"right_onto\" + 0.006*\"left_onto\" + 0.006*\"parking_restrictions\" + 0.005*\"zone_stopping\" + 0.005*\"avenue\" + 0.004*\"traffic\" + 0.004*\"police_special\" + 0.004*\"information\" + 0.004*\"place\" + 0.004*\"closed_traffic\" + 0.003*\"effect_follows\" + 0.003*\"parking\" + 0.003*\"route\" + 0.003*\"boylston\" + 0.003*\"city\" + 0.002*\"side\" + 0.002*\"charles\" + 0.002*\"around\"')\n"
     ]
    }
   ],
   "source": [
    "print(topic11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 inf inf inf inf inf inf inf inf inf inf inf inf \n",
      "inf 0.0 inf inf inf inf inf inf inf inf inf inf inf \n",
      "inf inf 0.0 inf inf inf inf inf inf inf inf inf inf \n",
      "inf inf inf 0.0 inf inf inf inf inf inf inf inf inf \n",
      "inf inf inf inf 0.0 inf inf inf inf inf inf inf inf \n",
      "inf inf inf inf inf 0.0 inf inf inf inf inf inf inf \n",
      "inf inf inf inf inf inf 0.0 inf inf inf inf inf inf \n",
      "inf inf inf inf inf inf inf 0.0 inf inf inf inf inf \n",
      "inf inf inf inf inf inf inf inf 0.0 inf inf inf inf \n",
      "inf inf inf inf inf inf inf inf inf 0.0 inf inf inf \n",
      "inf inf inf inf inf inf inf inf inf inf 0.0 inf inf \n",
      "inf inf inf inf inf inf inf inf inf inf inf 0.0 inf \n",
      "inf inf inf inf inf inf inf inf inf inf inf inf 0.0 \n"
     ]
    }
   ],
   "source": [
    "for topicA in Topics:\n",
    "    for topicB in Topics:\n",
    "        topic_A = make_topics_bow(topicA[1])\n",
    "        topic_B = make_topics_bow(topicB[1])\n",
    "        print(round(kullback_leibler(topic_A,topic_B),2), end = ' ')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
