{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import os\n",
    "import fnmatch\n",
    "import io\n",
    "from pprint import pprint\n",
    "from time import time  # to time our operations\n",
    "\n",
    "#LDA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os \n",
    "%matplotlib inline\n",
    "import pyLDAvis.gensim\n",
    "import gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess, tokenize\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "def stopwords_city(City):\n",
    "    month = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december' ]\n",
    "    day = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "    if City == 'Boston':\n",
    "        return month + day + ['boston', 'roxbury','fiandaca','opencounter', 'accela','forth', 'ortega', 'burro','theroxburyinnovation', 'julie', 'elliott', '21stcentury', 'nstar','dorsey','austin', 'blackmon', 'swett', 'ofboston', 'paired','ass','first', 'martin', 'addition', 'every', 'walsh', 'according', 'please visit', 'noted',\n",
    "            'please','april', 'february', 'visit', 'across', 'december', 'https', 'following',\n",
    "             'noted', 'along', 'three', 'total', 'pointed', 'citys', 'announced', 'massachusetts', 'million']\n",
    "    if City =='Taipei':\n",
    "        return month + day +['taipei','since', 'dadaocheng', 'serf', 'addition', 'february','related', 'october', 'january', 'around', 'taking', 'march', 'first', 'according', 'please visit',\n",
    "                'noted', 'please','april', 'among''february', 'visit', 'across', 'december', 'https', 'ko',\n",
    "                'following', 'universiade','september','november','august','noted', 'along', 'three', 'total', 'pointed', 'citys', 'chinese']\n",
    "    if City == 'Seoul':\n",
    "        return month + day +['seoul','soon','yeomchang', 'dulle','three', 'gaehwasinnonhyeon', 'major', 'first', 'million', 'various','selected','around','number', 'based','total',\n",
    "                'order','special', 'since']\n",
    "    if City == 'Helsinki':\n",
    "        return month + day + ['helsinki','based', '6aika', 'wrtsil','january', 'first', 'sitra']\n",
    "    else:\n",
    "        return month + day\n",
    "    \n",
    "def corpus(City):\n",
    "\n",
    "    folder = '/Users/clementnicolas/JournalPaper3/{}_News'.format(City)\n",
    "\n",
    "    corpus=[]\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords.extend(stopwords_city(City))\n",
    "    with_stopwords=0\n",
    "    without_stopwords=0\n",
    "    articles=0\n",
    "    tokens =0\n",
    "    for file in os.listdir(folder):\n",
    "        \n",
    "        path = os.path.join(folder, file)\n",
    "        if fnmatch.fnmatch(file, '*.txt'):\n",
    "            with io.open(path, 'r', encoding=\"UTF-8\") as fin:\n",
    "                content = fin.read()\n",
    "                data = content.splitlines()\n",
    "                #articles=articles+len(data)\n",
    "\n",
    "\n",
    "            for sentence in data:\n",
    "                word_list = tokenizer.tokenize(sentence.lower())\n",
    "                word_list1 = [word for word in word_list if word.isalpha()]\n",
    "                word_list2 = [word for word in word_list1 if len(word)>3]\n",
    "                word_list3 = [word for word in word_list2 if word not in stopwords]\n",
    "                if len(word_list3)>20:\n",
    "                    corpus.append(word_list3)\n",
    "                    #with_stopwords=with_stopwords+len(word_list2)\n",
    "                    #without_stopwords = without_stopwords+len(word_list3)\n",
    "                    #tokens = tokens+len(word_list)\n",
    "    \n",
    "    \n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(corpus, min_count=5, threshold=100)  # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[corpus], threshold=100)\n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    def make_bigrams(texts):\n",
    "        return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "    def make_trigrams(texts):\n",
    "        return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "    ######## For BIGRAMS\n",
    "    corpus2 = make_bigrams(corpus)\n",
    "    \n",
    "    ######## For TRIGRAMS\n",
    "    \n",
    "    #corpus22 = make_bigrams(corpus)\n",
    "    #corpus2 = make_trigrams(corpus22)\n",
    "\n",
    "    #print(corpus2[:10])\n",
    "    L=[]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for w in range(len(corpus2)):\n",
    "        x = 0\n",
    "        while x <(len(corpus2[w])):\n",
    "            #if corpus2[w][x] not in L:\n",
    "            #    L.append(corpus2[w][x])\n",
    "            corpus2[w][x] = lemmatizer.lemmatize(corpus2[w][x])\n",
    "            if corpus2[w][x] in stopwords:\n",
    "                del corpus2[w][x]\n",
    "            x += 1\n",
    "                \n",
    "    return corpus2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
